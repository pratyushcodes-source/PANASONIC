# ğŸ¤– Panasonic Self-Help Chatbot

An AI-powered chatbot that delivers instant, accurate answers to customer queries using Panasonic product manuals and user reviews. Designed to reduce customer support load and improve the self-service experience.

---

## ğŸ“Œ Table of Contents
- [Overview](#-project-overview)
- [Features](#-features)
- [Tech Stack](#-tech-stack)
- [Folder Structure](#-folder-structure)
- [Getting Started](#-getting-started)
- [Screenshots](#-screenshots)
- [Planned Enhancements](#-planned-enhancements)
- [License](#-license)
- [Acknowledgements](#-acknowledgements)

---

## ğŸš€ Project Overview

This project aims to create an intelligent self-help assistant for Panasonic product users. It reads product manuals (in PDF format), converts them into searchable knowledge using vector embeddings, and answers user questions via a React-based chat interface.

The chatbot:
- Loads content from Panasonic product manuals
- Uses **Ollama** (local LLM) for embeddings
- Leverages **ChromaDB** for fast semantic search
- Dynamically updates suggested follow-up questions based on user input

---

## âœ¨ Features

âœ… **PDF Parsing** â€“ Extracts structured content from Panasonic manuals  
âœ… **Vector Embedding** â€“ Converts text into semantic vectors for better recall  
âœ… **Local AI Models** â€“ Uses Ollama to generate context-aware answers  
âœ… **ChromaDB Integration** â€“ Efficient vector search for document-level context  
âœ… **Dynamic Suggestions** â€“ Follow-up questions auto-update as the user types  
âœ… **React Frontend** â€“ Modern, responsive chat UI  
âœ… **Modular Codebase** â€“ Easy to extend or swap components  

---

## ğŸ›  Tech Stack

| Layer        | Technology                  |
|--------------|-----------------------------|
| Frontend     | React + TypeScript          |
| Backend      | Node.js + Python (scripts)  |
| Vector DB    | ChromaDB (local)            |
| Embeddings   | Ollama (local LLM)          |
| Data Sources | Panasonic Manuals (PDF), Reviews |

---

<pre lang="markdown"> ## ğŸ“ Folder Structure
 ``` selfhelpbot-my-new-branch/ 
 â”œâ”€â”€ public/ # Static assets â”œâ”€â”€ 
 src/ 
 â”‚ â”œâ”€â”€ components/ # React components 
 â”‚ â”œâ”€â”€ services/ 
 â”‚ â”‚ â””â”€â”€ geminiService.ts # Gemini API integration (optional) 
 â”‚ â””â”€â”€ App.tsx # Main application 
 â”œâ”€â”€ data/ # Extracted product manual content 
 â”œâ”€â”€ scripts/ # Data loaders and vector embedding tools
 â”œâ”€â”€ chroma/ # ChromaDB local setup 
 â”œâ”€â”€ chroma_server.py # Starts ChromaDB service
 â”œâ”€â”€ search_server.py # Backend search API 
 â”œâ”€â”€ start_chroma_server.py # Combined startup script
 â”œâ”€â”€ package.json # Node.js dependencies 
 â””â”€â”€ README.md # Project documentation ``` </pre>



---

## âš™ï¸ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/pratyushcodes-source/PANASONIC.git
cd PANASONIC
2. Install Node.js Dependencies
bash
npm install
3. Start the Frontend
bash
npm run dev
4. Start the Chroma Search Server
bash
python start_chroma_server.py
âš ï¸ Note: Make sure you have Ollama installed and running locally with a supported model (e.g., llama3, mistral) before starting the vector search server.
Your PDF data should be preprocessed and embedded into the vector DB using your custom scripts.
 License
This project is developed for educational and internal research purposes only.
Please consult Panasonic before using it commercially or deploying it publicly.
---
ğŸ™Œ Acknowledgements
ğŸ“˜ Panasonic official manuals and support material

ğŸ§  Ollama â€” Run local LLMs

ğŸ—ƒ ChromaDB â€” Open-source vector DB

ğŸ”— Gemini API (experimental feature)

ğŸ’¼ Internship support and guidance from the Panasonic team (2025)


